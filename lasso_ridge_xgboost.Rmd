---
title: "SMC Final Project"
author: "Varshini Yanamandra"
date: "2023-04-16"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# required libraries
library(tidyverse)
library(glmnet)
library(readxl)
library(xgboost)
```

```{r}
# reading the data from an excel file
suppressWarnings({
  horses <- read_excel('HorseFavoriteDataset.xlsx')
})

head(horses)
```

```{r}
# creating a dataframe to hold all the accuracies
res = tibble(model = c("Ridge Regression", "LASSO Regression", "XGBoost"), accuracy = rep(0, 3))
```

```{r}
## ridge and lasso regression
set.seed(123)

# checking for null values
sum(is.na(horses)) # 0

# splitting the data into training (80%) and test(20%) datasets
train = sample(nrow(horses), nrow(horses) * 0.8) # training indices
length(train)/nrow(horses) # checking the split percentage

# creating model matrices
x_original = model.matrix(won ~ ., horses)[, -1]
# scaling the data
x = scale(x_original, center = TRUE, scale = TRUE)
y.tr = horses[train, ]$won
y.test = horses[-train, ]$won
```

```{r}
## ridge regression

# training and cross-validation
grid=10^seq(10, -2, length=100) # grid for lambdas
horses.ridge <- cv.glmnet(x[train, ], y.tr, alpha = 0, lambda = grid)
# prediction using the best lambda
horses.ridge.preds <- predict(horses.ridge, s = horses.ridge$lambda.min, newx = x[-train, ])
horses.ridge.preds <- ifelse(horses.ridge.preds < 0.5, 0, 1)
res[1, 2] = mean(horses.ridge.preds == y.test) # accuracy
# plot to see how MSE varies with lambda
plot(horses.ridge)
title("Ridge Regression", line = -0.01)

print(horses.ridge)
coef(horses.ridge)
```

```{r}
## lasso regression

# training and cross-validation
horses.lasso <- cv.glmnet(x[train, ], y.tr, alpha = 1, lambda = grid)
# prediction using the best lambda
horses.lasso.preds <- predict(horses.lasso, s = horses.lasso$lambda.min, newx = x[-train, ])
horses.lasso.preds <- ifelse(horses.lasso.preds < 0.5, 0, 1)
res[2, 2] = mean((horses.lasso.preds == y.test)) # accuracy
# plot to see how MSE varies with lambda
plot(horses.lasso)
title("LASSO Regression", line = -0.01)

print(horses.lasso)
coef(horses.lasso)
```

```{r}
## XGBoost
set.seed(123)

#define final training and testing sets
xgb_train <- xgb.DMatrix(data = x[train, ], label = y.tr)
xgb_test <- xgb.DMatrix(data = x[-train, ], label = y.test)

model <- xgboost(data = xgb_train, nround = 100, objective = "binary:logistic")

# generate predictions for our held-out testing data
pred <- predict(model, xgb_test)

# get the accuracy
res[3, 2] <- mean(as.numeric(pred > 0.5) == y.test)

# variable importance matrix
importance_matrix = xgb.importance(colnames(xgb_train), model = model)
importance_matrix

# plot variable importance
xgb.plot.importance(importance_matrix[1:5,])
```

```{r}
res
```

